{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bf643b772cea124",
   "metadata": {},
   "source": [
    "# Introduction to the Encoder in Attention Mechanism\n",
    "\n",
    "In this notebook, we will learn the fundamentals of the encoder and the attention mechanism using PyTorch. We will explore key concepts such as projection layers, dot products for attention scores, softmax for normalization, and weighted sums of values. By the end of this notebook, you'll have hands-on experience implementing these concepts and understanding how the encoder works in a neural network architecture like transformers.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183e79eac0409aea",
   "metadata": {},
   "source": [
    "## 1. Basic Linear Layer Exercise\n",
    "\n",
    "In the encoder, we use projection layers to project the input embeddings into a new space. This is done through a linear transformation.\n",
    "\n",
    "### Task:\n",
    "Implement a simple linear layer in PyTorch to project an input embedding into a new space (simulating the projection for Queries, Keys, or Values).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11f8833dc8b64113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\users\\utilizador\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.13.1)\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in c:\\users\\utilizador\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scipy) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: C:\\Users\\Utilizador\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'spftrf' from 'scipy.linalg.cython_lapack' (C:\\Users\\Utilizador\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\scipy\\linalg\\cython_lapack.cp311-win_amd64.pyd)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcython_lapack\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spftrf\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprophetnet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_prophetnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m softmax\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Example input: A batch of 3 sequences, each of length 4, with 5-dimensional embeddings\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'spftrf' from 'scipy.linalg.cython_lapack' (C:\\Users\\Utilizador\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\scipy\\linalg\\cython_lapack.cp311-win_amd64.pyd)"
     ]
    }
   ],
   "source": [
    "%pip install scipy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.linalg.cython_lapack import spftrf\n",
    "from transformers.models.prophetnet.modeling_prophetnet import softmax\n",
    "\n",
    "# Example input: A batch of 3 sequences, each of length 4, with 5-dimensional embeddings\n",
    "input_embeddings = torch.randn(4, 5)  # Shape: [batch_size, seq_length, embedding_dim]\n",
    "\n",
    "# Define a projection layer (linear transformation)\n",
    "projection_layer =  nn.Linear(5,6)  # Input size is 5, output size is 6\n",
    "\n",
    "# Apply projection layer to the input embeddings (like Q or K)\n",
    "projected = projection_layer(input_embeddings)\n",
    "print(projected.shape)  # Output should have shape: [4, 6]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94332b650bffed6",
   "metadata": {},
   "source": [
    "## 2. Dot Product for Attention Calculation\n",
    "In the attention mechanism, we calculate the similarity between the queries and keys using the dot product.\n",
    "\n",
    "### Task:\n",
    "Compute the similarity score between queries and keys using the dot product.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc124cdf84cf7d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example query and key vectors (after projection)\n",
    "queries = torch.randn(4, 6)  # Shape: [seq_length, d_k]\n",
    "keys = torch.randn(4, 6)     # Shape: [seq_length, d_k]\n",
    "\n",
    "attention_scores = torch.matmul(queries,keys.T)         #DOT PRODUCT HERE\n",
    "print(attention_scores.shape)  # Output: [4, 4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb1e19cdf68d5da",
   "metadata": {},
   "source": [
    "## 3. Softmax to Normalize Attention Scores\n",
    "The softmax function normalizes the attention scores, turning them into probabilities that sum to 1. This step is important for focusing the attention on the most relevant parts of the sequence.\n",
    "\n",
    "### Task:\n",
    "Apply softmax to the attention scores to get the attention weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbd0c1d7ffb35c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Example attention scores (e.g., dot product result)\n",
    "attention_scores = torch.randn(4, 4)  # Shape: [seq_length, seq_length]\n",
    "\n",
    "# Apply softmax to normalize the attention scores\n",
    "attention_weights = F.softmax(attention_scores,dim=1)   # Softmax along the last dimension\n",
    "print(attention_weights.shape)  # Output: [4, 4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fdf0a668167537",
   "metadata": {},
   "source": [
    "## 5. Full Attention Mechanism\n",
    "\n",
    "Now, we combine the previous steps to create the full scaled dot-product attention mechanism. This function computes the attention output by performing the following operations:\n",
    "\n",
    "1. Compute dot product between queries and keys.\n",
    "2. Apply softmax to normalize attention scores.\n",
    "3. Use the attention weights to compute the weighted sum of values.\n",
    "\n",
    "### Task:\n",
    "Implement the full attention mechanism by combining the previous operations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1673e08955955cdb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T15:25:02.348792Z",
     "start_time": "2025-01-20T15:25:02.344620Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2992342543.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    attention_scores = #DOT PRODUCT [seq_len, seq_len]\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Example input\n",
    "sentence_input = torch.randn(4, 6)  # [seq_len, d_k]\n",
    "d_k = 6\n",
    "\n",
    "# Example layers\n",
    "query_layer = nn.Linear(6, 6)\n",
    "key_layer = nn.Linear(6, 6)\n",
    "value_layer = nn.Linear(6, 6)\n",
    "\n",
    "# Apply the projections\n",
    "query = query_layer(sentence_input)   # Project input into query space\n",
    "keys = key_layer(sentence_input)  # Project input into key space\n",
    "values = value_layer(sentence_input)  # Project input into value space\n",
    "\n",
    "# Attention Scores: Scaled dot-product attention\n",
    "attention_scores = torch.matmul(query,keys.T)\n",
    "\n",
    "# Softmax to get attention weights\n",
    "attention_weights = F.softmax(attention_scores,dim=1)  # Normalize along last dimension USE: F.softmax\n",
    "\n",
    "# Final scores\n",
    "attention_output = torch.matmul(attention_weights, values)\n",
    "\n",
    "print(attention_output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
