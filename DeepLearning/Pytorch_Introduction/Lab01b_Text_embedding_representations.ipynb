{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def display_scatterplot(model, words):\n",
    "\n",
    "    if model.shape[1] == 2:\n",
    "        twodim = model\n",
    "    else:\n",
    "        twodim = PCA().fit_transform(model)[:,:2]\n",
    "    \n",
    "    plt.style.use('ggplot')\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
    "    for word, (x,y) in zip(words, twodim):\n",
    "        plt.text(x+0.05, y+0.05, word)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computation of Word Embeddings\n",
    "\n",
    "## Toy corpus"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "corpus = [\n",
    "    'she queen',\n",
    "    'she woman',\n",
    "    'he king',\n",
    "    'he man',\n",
    "    'london uk capital',\n",
    "    'lisbon portugal capital',   \n",
    "]\n",
    "\n",
    "def tokenize_corpus(corpus):\n",
    "    tokens = [x.split() for x in corpus]\n",
    "    return tokens\n",
    "\n",
    "tokenized_corpus = tokenize_corpus(corpus)\n",
    "print(tokenized_corpus)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "vocabulary = []\n",
    "for sentence in tokenized_corpus:\n",
    "    for token in sentence:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)\n",
    "\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(word2idx)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context windows"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "window_size = 2\n",
    "idx_pairs = []\n",
    "\n",
    "# for each sentence\n",
    "for sentence in tokenized_corpus:\n",
    "    sentence_indices = [word2idx[word] for word in sentence]\n",
    "    print(sentence)\n",
    "\n",
    "    # for each word, treated as center word\n",
    "    for center_word_pos in range(len(sentence_indices)):\n",
    "\n",
    "        # for each window position\n",
    "        for w in range(-window_size, window_size + 1):\n",
    "            context_word_pos = center_word_pos + w\n",
    "            # make sure not jump out sentence\n",
    "            if context_word_pos < 0 or context_word_pos >= len(sentence_indices) or center_word_pos == context_word_pos:\n",
    "                continue\n",
    "\n",
    "            context_word_idx = sentence_indices[context_word_pos]\n",
    "\n",
    "            print(\"POS:  \",idx2word[sentence_indices[center_word_pos]], \" \", idx2word[context_word_idx])\n",
    "            idx_pairs.append((sentence_indices[center_word_pos], context_word_idx, 1))\n",
    "\n",
    "## Negative sampling\n",
    "for word1 in vocabulary:\n",
    "    for word2 in vocabulary:\n",
    "        if((word2idx[word1], word2idx[word2], 1) not in idx_pairs):\n",
    "            print(\"NEG:  \",word1, \" \", word2)\n",
    "            idx_pairs.append((word2idx[word1], word2idx[word2], 0))\n",
    "\n",
    "idx_pairs = np.array(idx_pairs)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning cycle"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "embedding_dims = 4\n",
    "num_epochs = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "Wi = torch.randn(embedding_dims, vocabulary_size, requires_grad=True).float()\n",
    "Wo = torch.randn(vocabulary_size, embedding_dims, requires_grad=True).float()\n",
    "\n",
    "logreg = torch.nn.Sigmoid()\n",
    "loss = torch.nn.BCELoss()\n",
    "for epo in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    idx_pairs = np.random.permutation(idx_pairs)\n",
    "    for word_i, word_o, label in idx_pairs:\n",
    "        \n",
    "        # Get the current word-pair label\n",
    "        y_label = torch.from_numpy(np.array(label)).float()\n",
    "        \n",
    "        # Compute the condiditional probablity between the two words\n",
    "        z = torch.dot(Wi[:,word_i], Wo[word_o,:])\n",
    "        p_wo_wi = logreg(z)\n",
    "\n",
    "        # Compute the error with the positive/negative label\n",
    "        output = loss(p_wo_wi, y_label)\n",
    "        train_loss += output\n",
    "\n",
    "        # Propagate the error backward and update the parameters\n",
    "        output.backward()\n",
    "        Wi[:,word_i].data -= learning_rate * Wi.grad.data[:,word_i]\n",
    "        Wo[word_o,:].data -= learning_rate * Wo.grad.data[word_o,:]\n",
    "\n",
    "        # Reset\n",
    "        Wi.grad.data.zero_()\n",
    "        Wo.grad.data.zero_()\n",
    "                \n",
    "    if epo % 10 == 0:    \n",
    "        print(f'Loss at epo {epo}: {train_loss/len(idx_pairs)}')\n",
    "\n",
    "w2v = (Wi.T+Wo)/2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "display_scatterplot(w2v.detach().numpy(), vocabulary)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "w2v"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "\n",
    "# Get the interactive Tools for Matplotlib\n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load Word2Vec trained on Google News data\n",
    "wv = api.load('word2vec-google-news-300')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Import NumPy\n",
    "import numpy as np\n",
    "\n",
    "# Import PCA from scikit-learn\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Import PCA from scikit-learn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Import Gensim for the word embeddings\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "for i, word in enumerate(wv.key_to_index):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(word)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "vec_king = wv['king']\n",
    "print(vec_king)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words Similarity"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pairs = [\n",
    "    ('car', 'minivan'),   # a minivan is a kind of car\n",
    "    ('car', 'bicycle'),   # still a wheeled vehicle\n",
    "    ('car', 'airplane'),  # ok, no wheels, but still a vehicle\n",
    "    ('car', 'cereal'),    # ... and so on\n",
    "    ('car', 'communism'),\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(wv.most_similar(positive=['car', 'batman'], topn=5))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Analogies\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def analogy(x1, x2, y1):\n",
    "    result = wv.most_similar(positive=[y1, x2], negative=[x1])\n",
    "    return result[0][0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "analogy('japan', 'japanese', 'brazil')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "analogy('queen', 'king', 'woman')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "analogy('good', 'fantastic', 'bad')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def display_pca_scatterplot(model, words):\n",
    "\n",
    "    word_vectors = np.array([model[w] for w in words])\n",
    "\n",
    "    twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
    "    \n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
    "    for word, (x,y) in zip(words, twodim):\n",
    "        plt.text(x+0.05, y+0.05, word)\n",
    "        \n",
    "def display_closestwords(model, word):\n",
    "    \n",
    "    # get close words\n",
    "    a = model.similar_by_word(word)\n",
    "    close_words = [w for (w,s) in a]\n",
    "    close_words.append(word)\n",
    "    display_pca_scatterplot(model, close_words)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "display_pca_scatterplot(wv, \n",
    "                        ['coffee', 'tea', 'beer', 'wine', 'brandy', 'rum', 'champagne', 'water',\n",
    "                         'spaghetti', 'borscht', 'hamburger', 'pizza', 'falafel', 'sushi', 'meatballs',\n",
    "                         'dog', 'horse', 'cat', 'monkey', 'parrot', 'koala', 'lizard',\n",
    "                         'frog', 'toad', 'monkey', 'ape', 'kangaroo', 'wombat', 'wolf',\n",
    "                         'france', 'germany', 'hungary', 'france', 'australia', 'fiji', 'china',\n",
    "                         'homework', 'assignment', 'problem', 'exam', 'test', 'class',\n",
    "                         'school', 'college', 'university', 'institute'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "a = display_closestwords(wv, 'plane')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More\n",
    "\n",
    " - Word Embeddings for noisy text https://fasttext.cc/docs/en/english-vectors.html\n",
    "\n",
    "\n",
    " - Diachronic Word Embeddings https://nlp.stanford.edu/projects/histwords/\n",
    "\n",
    "\n",
    " - Sentence Embeddings https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
