{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40e4b959cb62141a",
   "metadata": {},
   "source": [
    "# Exercises Deep Learning\n",
    "First Lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f843372da6ee2f0",
   "metadata": {},
   "source": [
    "## Basic Tensor Operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f59e02c1a624f6b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T17:22:46.590064Z",
     "start_time": "2025-01-14T17:22:45.586928Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6b1beb874f2d53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor(2, 3, 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be071422e88e2797",
   "metadata": {},
   "source": [
    "Different ways to create tensors:\n",
    "- ```torch.zeros```: Creates a tensor filled with zeros\n",
    "- ```torch.ones```: Creates a tensor filled with ones\n",
    "- ```torch.rand```: Creates a tensor with random values uniformly sampled between 0 and 1\n",
    "- ```torch.randn```: Creates a tensor with random values sampled from a normal distribution with mean 0 and variance 1\n",
    "- ```torch.arange```: Creates a tensor containing the values\n",
    "- ```torch.Tensor``` (input list): Creates a tensor from the list elements you provide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652bfdd9d01d2cab",
   "metadata": {},
   "source": [
    "You can obtain the shape of a tensor in the same way as in numpy (```x.shape```), or using the ```.size``` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a5aba3abc981603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([2, 3, 4])\n",
      "Size: torch.Size([2, 3, 4])\n",
      "Size: 2 3 4\n"
     ]
    }
   ],
   "source": [
    "shape = x.shape\n",
    "print(\"Shape:\", x.shape)\n",
    "\n",
    "size = x.size()\n",
    "print(\"Size:\", size)\n",
    "\n",
    "dim1, dim2, dim3 = x.size()\n",
    "print(\"Size:\", dim1, dim2, dim3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1919de243a31992c",
   "metadata": {},
   "source": [
    "Tensor to Numpy, and Numpy to Tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "788bf302f6590378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy array: [[1 2]\n",
      " [3 4]]\n",
      "PyTorch tensor: tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "np_arr = np.array([[1, 2], [3, 4]])\n",
    "tensor = torch.from_numpy(np_arr)\n",
    "\n",
    "print(\"Numpy array:\", np_arr)\n",
    "print(\"PyTorch tensor:\", tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20199e8a5e016e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch tensor: tensor([0, 1, 2, 3])\n",
      "Numpy array: [0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.arange(4)\n",
    "np_arr = tensor.numpy()\n",
    "\n",
    "print(\"PyTorch tensor:\", tensor)\n",
    "print(\"Numpy array:\", np_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e78a94d4c6a5f4",
   "metadata": {},
   "source": [
    "Matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e97615c75ea0e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(6)\n",
    "x = x.view(2, 3)\n",
    "print(\"X\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8c106a45680668e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n"
     ]
    }
   ],
   "source": [
    "W = torch.arange(9).view(3, 3) # We can also stack multiple operations in a single line\n",
    "print(\"W\", W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1272a65d6ae9200e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h tensor([[15, 18, 21],\n",
      "        [42, 54, 66]])\n"
     ]
    }
   ],
   "source": [
    "h = torch.matmul(x, W) # Verify the result by calculating it by hand too!\n",
    "print(\"h\", h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721ec6774f26fb13",
   "metadata": {},
   "source": [
    " ### What about gpus?\n",
    "\n",
    "When you create a tensor the tensor is ready to be computed by the cpu. To convert the tensor you can use ```.to()```\n",
    "passing to the function \"cuda\" or \"cpu\" as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b57528335dc0269",
   "metadata": {},
   "source": [
    "#### How do I know if I have cuda cores on my computer?\n",
    "To solve this you can check with torch if cuda is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38113a46a272b80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available. Using CPU for PyTorch.\n"
     ]
    }
   ],
   "source": [
    "example_tensor = torch.rand(2,2)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. You can use GPU for PyTorch.\")\n",
    "    example_tensor.to(\"cuda\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU for PyTorch.\")\n",
    "    example_tensor.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8127a2bc83ad170a",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbe4a9ddaf9b81d",
   "metadata": {},
   "source": [
    "#### 1. Create two tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964101079da9badd",
   "metadata": {},
   "source": [
    "   - A 3x3 tensor of random numbers.\n",
    "   - A 3x3 tensor filled with ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83e1b2cdb8bade78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5755, 0.9911, 0.1409],\n",
      "        [0.6355, 0.2189, 0.8860],\n",
      "        [0.3453, 0.1567, 0.6224]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "#Exercise 1\n",
    "\n",
    "randTensor = torch.rand(3, 3)\n",
    "oneTensor = torch.ones(3, 3)\n",
    "\n",
    "print(randTensor)\n",
    "print(oneTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a271b3d5fb87c50b",
   "metadata": {},
   "source": [
    "#### 2. Perform the following operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db794d8774eae177",
   "metadata": {},
   "source": [
    "- Add the two tensors.\n",
    "- Multiply the two tensors element-wise.\n",
    "- Compute the dot product between the first row of both tensors.\n",
    " - Find the transpose of the resulting tensor from the element-wise multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "406bfe0aed2719b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5755, 1.9911, 1.1409],\n",
      "        [1.6355, 1.2189, 1.8860],\n",
      "        [1.3453, 1.1567, 1.6224]])\n",
      "tensor([[0.5755, 0.9911, 0.1409],\n",
      "        [0.6355, 0.2189, 0.8860],\n",
      "        [0.3453, 0.1567, 0.6224]])\n",
      "tensor(4.5723)\n",
      "tensor([[0.5755, 0.6355, 0.3453],\n",
      "        [0.9911, 0.2189, 0.1567],\n",
      "        [0.1409, 0.8860, 0.6224]])\n"
     ]
    }
   ],
   "source": [
    "#Exercise 2\n",
    "sum =  torch.add(randTensor, oneTensor)\n",
    "print(sum)\n",
    "\n",
    "mul = torch.mul(randTensor, oneTensor)\n",
    "print(mul)\n",
    "\n",
    "dotP = torch.dot(randTensor.view(-1), oneTensor.view(-1))\n",
    "print(dotP)\n",
    "\n",
    "transpose = torch.transpose(mul, 0, 1)\n",
    "print(transpose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57afe90005dc29a",
   "metadata": {},
   "source": [
    "#### 3. Convert the resulting tensor to a NumPy array and back to a PyTorch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "182f70f0737cff3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5754898  0.6354525  0.34534413]\n",
      " [0.9911484  0.21891862 0.15665454]\n",
      " [0.14086848 0.8860174  0.6224009 ]]\n",
      "tensor([[0.5755, 0.6355, 0.3453],\n",
      "        [0.9911, 0.2189, 0.1567],\n",
      "        [0.1409, 0.8860, 0.6224]])\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3\n",
    "\n",
    "npArray = transpose.numpy()\n",
    "print(npArray)\n",
    "\n",
    "transAgain = torch.from_numpy(npArray)\n",
    "print(transAgain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589117b4a2e3f7cb",
   "metadata": {},
   "source": [
    "## Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16d9efcf227bb68",
   "metadata": {},
   "source": [
    "1. Create Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be5673134b6e6eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_a = torch.tensor(0., requires_grad=True)\n",
    "x_b = torch.tensor(0., requires_grad=True)\n",
    "w_a = torch.tensor(0.9, requires_grad=True)\n",
    "w_b = torch.tensor(0.9, requires_grad=True)\n",
    "\n",
    "y = torch.tensor(0., requires_grad=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db0554422da0a80",
   "metadata": {},
   "source": [
    "2. Build a computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35fad594fa32a16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_a = w_a * x_a\n",
    "weighted_b = w_b * x_b\n",
    "sum_unit = weighted_a + weighted_b\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b928ece0981eb0",
   "metadata": {},
   "source": [
    "3. Activation Function\n",
    "\n",
    "For a simple approach as ease of replication by hand we will this activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc171048fa0f693d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5000, grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = torch.sigmoid(sum_unit)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25082a0d8778158f",
   "metadata": {},
   "source": [
    "4. Calculate Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fea378f48b561f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.BCELoss()\n",
    "output = loss(y_hat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4b00ba9c0d510c",
   "metadata": {},
   "source": [
    "5. Calculate gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59e67123bb0c02f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2607801d848cb5a",
   "metadata": {},
   "source": [
    "6.Print out the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe7b458d47eeedab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4500)\n",
      "tensor(0.4500)\n",
      "tensor(0.)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "print(x_a.grad)\n",
    "print(x_b.grad)\n",
    "print(w_a.grad)\n",
    "print(w_b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e479bfbd78c7d3e7",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e7d26b57cd49c3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T17:22:49.794682Z",
     "start_time": "2025-01-14T17:22:49.788011Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "epochs = 100\n",
    "\n",
    "input_data = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "target_data = torch.tensor([[0], [0], [0], [1]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "122d9dbb9d32cab2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T17:22:50.655971Z",
     "start_time": "2025-01-14T17:22:50.652999Z"
    }
   },
   "outputs": [],
   "source": [
    "class ANDGateModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ANDGateModel, self).__init__()\n",
    "        self.linear = nn.Linear(2, 1,bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99ae9c0e028670fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T17:24:38.411499Z",
     "start_time": "2025-01-14T17:24:38.375069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100, Loss: 0.0972\n",
      "Final weights: tensor([[3.9964, 3.9329]])\n",
      "Final bias: tensor([-5.9941])\n",
      "Input: [0. 0.] -> Predicted Output: 0, Raw Output: 0.0025\n",
      "Input: [0. 1.] -> Predicted Output: 0, Raw Output: 0.1129\n",
      "Input: [1. 0.] -> Predicted Output: 0, Raw Output: 0.1194\n",
      "Input: [1. 1.] -> Predicted Output: 1, Raw Output: 0.8738\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = ANDGateModel()\n",
    "\n",
    "# Loss function (Binary Cross-Entropy Loss)\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "# Optimizer (Stochastic Gradient Descent)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    y_hat = model(input_data)\n",
    "    loss = loss_fn(y_hat, target_data)\n",
    "\n",
    "\n",
    "    loss.backward() # Backpropagation\n",
    "    optimizer.step() # Update parameters using the optimizer\n",
    "    optimizer.zero_grad() # Zero the gradients for the next iteration\n",
    "\n",
    "    # Print loss and progress every 1000 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Final weights and bias (optional)\n",
    "print(f\"Final weights: {model.linear.weight.data}\")\n",
    "print(f\"Final bias: {model.linear.bias.data}\")\n",
    "\n",
    "# Test the AND gate\n",
    "with torch.no_grad():\n",
    "    for i in range(len(input_data)):\n",
    "        x_a, x_b = input_data[i]\n",
    "        y_hat = model(torch.tensor([[x_a, x_b]]))  # Model expects a batch\n",
    "        print(f\"Input: {input_data[i].numpy()} -> Predicted Output: {round(y_hat.item())}, Raw Output: {y_hat.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18785f163f5d7904",
   "metadata": {},
   "source": [
    "!!! IMPORTANT: This example has a significant issue: the test set is the same as the training set.\n",
    "This approach is used here solely for ease of explanation and should never be used in a production environment.!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d64492b0a9f095",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a64599701c601b",
   "metadata": {},
   "source": [
    "#### 1.Replicate the OR Gate using a Neural Network\n",
    " Objective:\n",
    "- Train a neural network to approximate the function of an OR gate.\n",
    "- Compare how changing the weights or biases impacts the output of the network.\n",
    "\n",
    "Input 1 | Input 2 | Output (OR)\n",
    "| -- | -- | --|\n",
    "0 | 0 | 0\n",
    "0 | 1 | 1\n",
    "1 | 0 | 1\n",
    "1 | 1 | 1\n",
    "\n",
    "1. Create the dataset\n",
    "2. Replicate the architecture from the AND gate example\n",
    "3. Change the loss function from Binary Cross-Entropy to Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a197f7d767d46a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Here\n",
    "\n",
    "class ORGateModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ORGateModel, self).__init__()\n",
    "        self.linear = nn.Linear(2, 1,bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f1dcfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data = torch.tensor([[0], [1], [1], [1]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c033bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100, Loss: 0.0085\n",
      "Final weights: tensor([[4.2707, 4.1543]])\n",
      "Final bias: tensor([-1.8241])\n",
      "Input: [0. 0.] -> Predicted Output: 0, Raw Output: 0.1389\n",
      "Input: [0. 1.] -> Predicted Output: 1, Raw Output: 0.9113\n",
      "Input: [1. 0.] -> Predicted Output: 1, Raw Output: 0.9203\n",
      "Input: [1. 1.] -> Predicted Output: 1, Raw Output: 0.9986\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = ORGateModel()\n",
    "\n",
    "# Loss function (Binary Cross-Entropy Loss)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Optimizer (Stochastic Gradient Descent)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    y_hat = model(input_data)\n",
    "    loss = loss_fn(y_hat, target_data)\n",
    "\n",
    "\n",
    "    loss.backward() # Backpropagation\n",
    "    optimizer.step() # Update parameters using the optimizer\n",
    "    optimizer.zero_grad() # Zero the gradients for the next iteration\n",
    "\n",
    "    # Print loss and progress every 1000 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Final weights and bias (optional)\n",
    "print(f\"Final weights: {model.linear.weight.data}\")\n",
    "print(f\"Final bias: {model.linear.bias.data}\")\n",
    "\n",
    "# Test the AND gate\n",
    "with torch.no_grad():\n",
    "    for i in range(len(input_data)):\n",
    "        x_a, x_b = input_data[i]\n",
    "        y_hat = model(torch.tensor([[x_a, x_b]]))  # Model expects a batch\n",
    "        print(f\"Input: {input_data[i].numpy()} -> Predicted Output: {round(y_hat.item())}, Raw Output: {y_hat.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c401a9cd9bd0be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fc0ab91a8ed899",
   "metadata": {},
   "source": [
    "#### 2. Build and train a network\n",
    "1. Build a simple fully connected neural network with the following architecture:\n",
    "    - Input layer with 2 units\n",
    "    - Hidden layer with 4 units and ReLU activation\n",
    "    - Output layer with 1 unit\n",
    "2. Define the following loss function and optimizer:\n",
    "    - Loss: Mean Squared Error (MSE)\n",
    "    - Optimizer: Stochastic Gradient Descent (SGD)\n",
    "\n",
    "The network should mimic $y = 2x_1 + 3x_2$, where $x_1$ and $x_2$ are random inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8faceb73b3024932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 5.473449230194092\n",
      "Epoch 20, Loss: 2.632601261138916\n",
      "Epoch 30, Loss: 1.1225172281265259\n",
      "Epoch 40, Loss: 0.6394160389900208\n",
      "Epoch 50, Loss: 0.534110963344574\n",
      "Epoch 60, Loss: 0.5026615262031555\n",
      "Epoch 70, Loss: 0.48144975304603577\n",
      "Epoch 80, Loss: 0.4615555703639984\n",
      "Epoch 90, Loss: 0.44194427132606506\n",
      "Epoch 100, Loss: 0.42252400517463684\n",
      "Epoch 110, Loss: 0.4033094048500061\n",
      "Epoch 120, Loss: 0.3843275308609009\n",
      "Epoch 130, Loss: 0.36560824513435364\n",
      "Epoch 140, Loss: 0.34718233346939087\n",
      "Epoch 150, Loss: 0.32908087968826294\n",
      "Epoch 160, Loss: 0.3113352358341217\n",
      "Epoch 170, Loss: 0.2939767837524414\n",
      "Epoch 180, Loss: 0.27703624963760376\n",
      "Epoch 190, Loss: 0.2605436146259308\n",
      "Epoch 200, Loss: 0.2445271760225296\n",
      "Epoch 210, Loss: 0.22901363670825958\n",
      "Epoch 220, Loss: 0.2140275239944458\n",
      "Epoch 230, Loss: 0.1995907723903656\n",
      "Epoch 240, Loss: 0.18572242558002472\n",
      "Epoch 250, Loss: 0.17243848741054535\n",
      "Epoch 260, Loss: 0.15975160896778107\n",
      "Epoch 270, Loss: 0.1476708948612213\n",
      "Epoch 280, Loss: 0.13620193302631378\n",
      "Epoch 290, Loss: 0.12534670531749725\n",
      "Epoch 300, Loss: 0.115103580057621\n",
      "Epoch 310, Loss: 0.10546735674142838\n",
      "Epoch 320, Loss: 0.09642962366342545\n",
      "Epoch 330, Loss: 0.08797876536846161\n",
      "Epoch 340, Loss: 0.0801004096865654\n",
      "Epoch 350, Loss: 0.0727774128317833\n",
      "Epoch 360, Loss: 0.06599058955907822\n",
      "Epoch 370, Loss: 0.05971875041723251\n",
      "Epoch 380, Loss: 0.05393904820084572\n",
      "Epoch 390, Loss: 0.04862770438194275\n",
      "Epoch 400, Loss: 0.0437597818672657\n",
      "Epoch 410, Loss: 0.039309944957494736\n",
      "Epoch 420, Loss: 0.03525266796350479\n",
      "Epoch 430, Loss: 0.03156241029500961\n",
      "Epoch 440, Loss: 0.02821391448378563\n",
      "Epoch 450, Loss: 0.025182560086250305\n",
      "Epoch 460, Loss: 0.0224443469196558\n",
      "Epoch 470, Loss: 0.01997612789273262\n",
      "Epoch 480, Loss: 0.017755843698978424\n",
      "Epoch 490, Loss: 0.015763700008392334\n",
      "Epoch 500, Loss: 0.01398038025945425\n",
      "Epoch 510, Loss: 0.012387474998831749\n",
      "Epoch 520, Loss: 0.010965278372168541\n",
      "Epoch 530, Loss: 0.009698241949081421\n",
      "Epoch 540, Loss: 0.008569852448999882\n",
      "Epoch 550, Loss: 0.0075668226927518845\n",
      "Epoch 560, Loss: 0.006676637567579746\n",
      "Epoch 570, Loss: 0.005887978710234165\n",
      "Epoch 580, Loss: 0.005190449766814709\n",
      "Epoch 590, Loss: 0.0045739938504993916\n",
      "Epoch 600, Loss: 0.0040291594341397285\n",
      "Epoch 610, Loss: 0.0035486482083797455\n",
      "Epoch 620, Loss: 0.003125118790194392\n",
      "Epoch 630, Loss: 0.002752295695245266\n",
      "Epoch 640, Loss: 0.0024243027437478304\n",
      "Epoch 650, Loss: 0.002135906834155321\n",
      "Epoch 660, Loss: 0.0018823744030669332\n",
      "Epoch 670, Loss: 0.0016595908673480153\n",
      "Epoch 680, Loss: 0.0014639169676229358\n",
      "Epoch 690, Loss: 0.0012919310247525573\n",
      "Epoch 700, Loss: 0.0011408361606299877\n",
      "Epoch 710, Loss: 0.0010083091910928488\n",
      "Epoch 720, Loss: 0.0008920904365368187\n",
      "Epoch 730, Loss: 0.0007903799996711314\n",
      "Epoch 740, Loss: 0.0007011569105088711\n",
      "Epoch 750, Loss: 0.0006228603888303041\n",
      "Epoch 760, Loss: 0.000554171041585505\n",
      "Epoch 770, Loss: 0.0004939873469993472\n",
      "Epoch 780, Loss: 0.0004412041453178972\n",
      "Epoch 790, Loss: 0.0003949272504542023\n",
      "Epoch 800, Loss: 0.0003543722559697926\n",
      "Epoch 810, Loss: 0.0003188079281244427\n",
      "Epoch 820, Loss: 0.00028769587515853345\n",
      "Epoch 830, Loss: 0.00026044403784908354\n",
      "Epoch 840, Loss: 0.00023655325639992952\n",
      "Epoch 850, Loss: 0.0002155891852453351\n",
      "Epoch 860, Loss: 0.00019719796546269208\n",
      "Epoch 870, Loss: 0.00018105834897141904\n",
      "Epoch 880, Loss: 0.00016689542098902166\n",
      "Epoch 890, Loss: 0.00015446587349288166\n",
      "Epoch 900, Loss: 0.00014355765597429127\n",
      "Epoch 910, Loss: 0.00013398258306551725\n",
      "Epoch 920, Loss: 0.00012557831360027194\n",
      "Epoch 930, Loss: 0.00011819782957900316\n",
      "Epoch 940, Loss: 0.00011171491496497765\n",
      "Epoch 950, Loss: 0.00010601984831737354\n",
      "Epoch 960, Loss: 0.00010101633233716711\n",
      "Epoch 970, Loss: 9.662144293542951e-05\n",
      "Epoch 980, Loss: 9.275491174776107e-05\n",
      "Epoch 990, Loss: 8.935436198953539e-05\n",
      "Epoch 1000, Loss: 8.636287384433672e-05\n",
      "Epoch 1010, Loss: 8.37274783407338e-05\n",
      "Epoch 1020, Loss: 8.140400314005092e-05\n",
      "Epoch 1030, Loss: 7.935520261526108e-05\n",
      "Epoch 1040, Loss: 7.754581019980833e-05\n",
      "Epoch 1050, Loss: 7.59478280087933e-05\n",
      "Epoch 1060, Loss: 7.4534495070111e-05\n",
      "Epoch 1070, Loss: 7.328305946430191e-05\n",
      "Epoch 1080, Loss: 7.217418897198513e-05\n",
      "Epoch 1090, Loss: 7.119077781680971e-05\n",
      "Epoch 1100, Loss: 7.031623681541532e-05\n",
      "Epoch 1110, Loss: 6.953880074433982e-05\n",
      "Epoch 1120, Loss: 6.884552567498758e-05\n",
      "Epoch 1130, Loss: 6.822648720117286e-05\n",
      "Epoch 1140, Loss: 6.767259037587792e-05\n",
      "Epoch 1150, Loss: 6.717593350913376e-05\n",
      "Epoch 1160, Loss: 6.672961171716452e-05\n",
      "Epoch 1170, Loss: 6.632761505898088e-05\n",
      "Epoch 1180, Loss: 6.596429011551663e-05\n",
      "Epoch 1190, Loss: 6.563516944879666e-05\n",
      "Epoch 1200, Loss: 6.533615669468418e-05\n",
      "Epoch 1210, Loss: 6.506353383883834e-05\n",
      "Epoch 1220, Loss: 6.481407035607845e-05\n",
      "Epoch 1230, Loss: 6.458508141804487e-05\n",
      "Epoch 1240, Loss: 6.437408592319116e-05\n",
      "Epoch 1250, Loss: 6.417879922082648e-05\n",
      "Epoch 1260, Loss: 6.399770063580945e-05\n",
      "Epoch 1270, Loss: 6.382889114320278e-05\n",
      "Epoch 1280, Loss: 6.367104651872069e-05\n",
      "Epoch 1290, Loss: 6.352263881126419e-05\n",
      "Epoch 1300, Loss: 6.338278035400435e-05\n",
      "Epoch 1310, Loss: 6.325032882159576e-05\n",
      "Epoch 1320, Loss: 6.312444020295516e-05\n",
      "Epoch 1330, Loss: 6.300443055806682e-05\n",
      "Epoch 1340, Loss: 6.288960139499977e-05\n",
      "Epoch 1350, Loss: 6.277934880927205e-05\n",
      "Epoch 1360, Loss: 6.267309800023213e-05\n",
      "Epoch 1370, Loss: 6.257044151425362e-05\n",
      "Epoch 1380, Loss: 6.247095006983727e-05\n",
      "Epoch 1390, Loss: 6.23743690084666e-05\n",
      "Epoch 1400, Loss: 6.228019628906623e-05\n",
      "Epoch 1410, Loss: 6.21883082203567e-05\n",
      "Epoch 1420, Loss: 6.209834828041494e-05\n",
      "Epoch 1430, Loss: 6.201026553753763e-05\n",
      "Epoch 1440, Loss: 6.192369619384408e-05\n",
      "Epoch 1450, Loss: 6.183856021380052e-05\n",
      "Epoch 1460, Loss: 6.175466842250898e-05\n",
      "Epoch 1470, Loss: 6.167191168060526e-05\n",
      "Epoch 1480, Loss: 6.159023905638605e-05\n",
      "Epoch 1490, Loss: 6.150939589133486e-05\n",
      "Epoch 1500, Loss: 6.142941856523976e-05\n",
      "Epoch 1510, Loss: 6.135022704256698e-05\n",
      "Epoch 1520, Loss: 6.12715957686305e-05\n",
      "Epoch 1530, Loss: 6.119369209045544e-05\n",
      "Epoch 1540, Loss: 6.111623224569485e-05\n",
      "Epoch 1550, Loss: 6.103937630541623e-05\n",
      "Epoch 1560, Loss: 6.096297511248849e-05\n",
      "Epoch 1570, Loss: 6.088697045925073e-05\n",
      "Epoch 1580, Loss: 6.081134779378772e-05\n",
      "Epoch 1590, Loss: 6.073614349588752e-05\n",
      "Epoch 1600, Loss: 6.066132482374087e-05\n",
      "Epoch 1610, Loss: 6.058668077457696e-05\n",
      "Epoch 1620, Loss: 6.05124187131878e-05\n",
      "Epoch 1630, Loss: 6.043848043191247e-05\n",
      "Epoch 1640, Loss: 6.036472041159868e-05\n",
      "Epoch 1650, Loss: 6.0291287809377536e-05\n",
      "Epoch 1660, Loss: 6.021801891620271e-05\n",
      "Epoch 1670, Loss: 6.0144910094095394e-05\n",
      "Epoch 1680, Loss: 6.007211050018668e-05\n",
      "Epoch 1690, Loss: 5.999959830660373e-05\n",
      "Epoch 1700, Loss: 5.99271988903638e-05\n",
      "Epoch 1710, Loss: 5.985486495774239e-05\n",
      "Epoch 1720, Loss: 5.97828438912984e-05\n",
      "Epoch 1730, Loss: 5.9710997447837144e-05\n",
      "Epoch 1740, Loss: 5.963932198937982e-05\n",
      "Epoch 1750, Loss: 5.956777749815956e-05\n",
      "Epoch 1760, Loss: 5.9496353060239926e-05\n",
      "Epoch 1770, Loss: 5.9425190556794405e-05\n",
      "Epoch 1780, Loss: 5.935402441537008e-05\n",
      "Epoch 1790, Loss: 5.928310929448344e-05\n",
      "Epoch 1800, Loss: 5.921233241679147e-05\n",
      "Epoch 1810, Loss: 5.9141773817827925e-05\n",
      "Epoch 1820, Loss: 5.9071247960673645e-05\n",
      "Epoch 1830, Loss: 5.900090036448091e-05\n",
      "Epoch 1840, Loss: 5.8930698287440464e-05\n",
      "Epoch 1850, Loss: 5.886058715987019e-05\n",
      "Epoch 1860, Loss: 5.879063610336743e-05\n",
      "Epoch 1870, Loss: 5.872084875591099e-05\n",
      "Epoch 1880, Loss: 5.865126149728894e-05\n",
      "Epoch 1890, Loss: 5.858175063622184e-05\n",
      "Epoch 1900, Loss: 5.8512458053883165e-05\n",
      "Epoch 1910, Loss: 5.844317638548091e-05\n",
      "Epoch 1920, Loss: 5.837407661601901e-05\n",
      "Epoch 1930, Loss: 5.830511872773059e-05\n",
      "Epoch 1940, Loss: 5.8236309996573254e-05\n",
      "Epoch 1950, Loss: 5.816759949084371e-05\n",
      "Epoch 1960, Loss: 5.809901267639361e-05\n",
      "Epoch 1970, Loss: 5.803060412290506e-05\n",
      "Epoch 1980, Loss: 5.796226469101384e-05\n",
      "Epoch 1990, Loss: 5.78940671402961e-05\n",
      "Epoch 2000, Loss: 5.7826062402455136e-05\n",
      "Epoch 2010, Loss: 5.7758075854508206e-05\n",
      "Epoch 2020, Loss: 5.7690293033374473e-05\n",
      "Epoch 2030, Loss: 5.762262298958376e-05\n",
      "Epoch 2040, Loss: 5.755500023951754e-05\n",
      "Epoch 2050, Loss: 5.74876248720102e-05\n",
      "Epoch 2060, Loss: 5.7420282246312127e-05\n",
      "Epoch 2070, Loss: 5.735307058785111e-05\n",
      "Epoch 2080, Loss: 5.7286062656203285e-05\n",
      "Epoch 2090, Loss: 5.721906927647069e-05\n",
      "Epoch 2100, Loss: 5.71522323298268e-05\n",
      "Epoch 2110, Loss: 5.7085591834038496e-05\n",
      "Epoch 2120, Loss: 5.701900226995349e-05\n",
      "Epoch 2130, Loss: 5.695256913895719e-05\n",
      "Epoch 2140, Loss: 5.688619057764299e-05\n",
      "Epoch 2150, Loss: 5.6819975725375116e-05\n",
      "Epoch 2160, Loss: 5.6753917306195945e-05\n",
      "Epoch 2170, Loss: 5.668793528457172e-05\n",
      "Epoch 2180, Loss: 5.662216426571831e-05\n",
      "Epoch 2190, Loss: 5.6556400522822514e-05\n",
      "Epoch 2200, Loss: 5.64908332307823e-05\n",
      "Epoch 2210, Loss: 5.642534961225465e-05\n",
      "Epoch 2220, Loss: 5.6359982409048826e-05\n",
      "Epoch 2230, Loss: 5.629475708701648e-05\n",
      "Epoch 2240, Loss: 5.6229589972645044e-05\n",
      "Epoch 2250, Loss: 5.6164593843277544e-05\n",
      "Epoch 2260, Loss: 5.609963045571931e-05\n",
      "Epoch 2270, Loss: 5.6034899898804724e-05\n",
      "Epoch 2280, Loss: 5.5970216635614634e-05\n",
      "Epoch 2290, Loss: 5.590569708147086e-05\n",
      "Epoch 2300, Loss: 5.584122845903039e-05\n",
      "Epoch 2310, Loss: 5.5776938097551465e-05\n",
      "Epoch 2320, Loss: 5.5712731409585103e-05\n",
      "Epoch 2330, Loss: 5.564861203311011e-05\n",
      "Epoch 2340, Loss: 5.558465636568144e-05\n",
      "Epoch 2350, Loss: 5.552080619963817e-05\n",
      "Epoch 2360, Loss: 5.545706153498031e-05\n",
      "Epoch 2370, Loss: 5.539339690585621e-05\n",
      "Epoch 2380, Loss: 5.532984505407512e-05\n",
      "Epoch 2390, Loss: 5.5266384151764214e-05\n",
      "Epoch 2400, Loss: 5.5203097872436047e-05\n",
      "Epoch 2410, Loss: 5.513981886906549e-05\n",
      "Epoch 2420, Loss: 5.5076707212720066e-05\n",
      "Epoch 2430, Loss: 5.5013690143823624e-05\n",
      "Epoch 2440, Loss: 5.495080040418543e-05\n",
      "Epoch 2450, Loss: 5.488800525199622e-05\n",
      "Epoch 2460, Loss: 5.4825326515128836e-05\n",
      "Epoch 2470, Loss: 5.476276419358328e-05\n",
      "Epoch 2480, Loss: 5.470027099363506e-05\n",
      "Epoch 2490, Loss: 5.463788693305105e-05\n",
      "Epoch 2500, Loss: 5.4575612011831254e-05\n",
      "Epoch 2510, Loss: 5.451347169582732e-05\n",
      "Epoch 2520, Loss: 5.445146598503925e-05\n",
      "Epoch 2530, Loss: 5.4389540309784934e-05\n",
      "Epoch 2540, Loss: 5.4327731049852446e-05\n",
      "Epoch 2550, Loss: 5.4266049119178206e-05\n",
      "Epoch 2560, Loss: 5.4204454499995336e-05\n",
      "Epoch 2570, Loss: 5.414298357209191e-05\n",
      "Epoch 2580, Loss: 5.408154902397655e-05\n",
      "Epoch 2590, Loss: 5.402027818490751e-05\n",
      "Epoch 2600, Loss: 5.39591746928636e-05\n",
      "Epoch 2610, Loss: 5.389812577050179e-05\n",
      "Epoch 2620, Loss: 5.383716052165255e-05\n",
      "Epoch 2630, Loss: 5.377636989578605e-05\n",
      "Epoch 2640, Loss: 5.371558654587716e-05\n",
      "Epoch 2650, Loss: 5.365502875065431e-05\n",
      "Epoch 2660, Loss: 5.359445640351623e-05\n",
      "Epoch 2670, Loss: 5.353407686925493e-05\n",
      "Epoch 2680, Loss: 5.347377009456977e-05\n",
      "Epoch 2690, Loss: 5.3413587011164054e-05\n",
      "Epoch 2700, Loss: 5.335352761903778e-05\n",
      "Epoch 2710, Loss: 5.3293522796593606e-05\n",
      "Epoch 2720, Loss: 5.323367440723814e-05\n",
      "Epoch 2730, Loss: 5.317377144820057e-05\n",
      "Epoch 2740, Loss: 5.311414133757353e-05\n",
      "Epoch 2750, Loss: 5.305453669279814e-05\n",
      "Epoch 2760, Loss: 5.2995059377280995e-05\n",
      "Epoch 2770, Loss: 5.2935680287191644e-05\n",
      "Epoch 2780, Loss: 5.287639214657247e-05\n",
      "Epoch 2790, Loss: 5.2817205869359896e-05\n",
      "Epoch 2800, Loss: 5.275810690363869e-05\n",
      "Epoch 2810, Loss: 5.2699182560900226e-05\n",
      "Epoch 2820, Loss: 5.264036008156836e-05\n",
      "Epoch 2830, Loss: 5.258161763777025e-05\n",
      "Epoch 2840, Loss: 5.252298433333635e-05\n",
      "Epoch 2850, Loss: 5.246438013273291e-05\n",
      "Epoch 2860, Loss: 5.240600148681551e-05\n",
      "Epoch 2870, Loss: 5.234768832451664e-05\n",
      "Epoch 2880, Loss: 5.228940790402703e-05\n",
      "Epoch 2890, Loss: 5.2231298468541354e-05\n",
      "Epoch 2900, Loss: 5.217326543061063e-05\n",
      "Epoch 2910, Loss: 5.211526149651036e-05\n",
      "Epoch 2920, Loss: 5.205751585890539e-05\n",
      "Epoch 2930, Loss: 5.199979932513088e-05\n",
      "Epoch 2940, Loss: 5.194214099901728e-05\n",
      "Epoch 2950, Loss: 5.1884671847801656e-05\n",
      "Epoch 2960, Loss: 5.1827257266268134e-05\n",
      "Epoch 2970, Loss: 5.176989361643791e-05\n",
      "Epoch 2980, Loss: 5.171270458959043e-05\n",
      "Epoch 2990, Loss: 5.1655570132425055e-05\n",
      "Epoch 3000, Loss: 5.1598555728560314e-05\n",
      "Test Loss: 6.627844413742423e-05\n",
      "Final Predictions (first 5 test samples):\n",
      "Predicted: 2.5445, Actual: 2.5448\n",
      "Predicted: 2.6783, Actual: 2.6782\n",
      "Predicted: 3.1351, Actual: 3.1351\n",
      "Predicted: 3.8501, Actual: 3.8495\n",
      "Predicted: 1.9276, Actual: 1.9273\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the neural network\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        # Define layers here\n",
    "        self.layer1 =   nn.Linear(2, 4)\n",
    "        self.activation_function = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(4, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define forward pass\n",
    "        x = self.layer1(x)\n",
    "        x = self.activation_function(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "# Create synthetic data\n",
    "x = torch.rand(10000, 2)\n",
    "y = 2 * x[:, 0] + 3 * x[:, 1]\n",
    "y = y.view(-1, 1)\n",
    "\n",
    "# Split data into training and test sets (80% train, 20% test)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleNet()\n",
    "criterion =  torch.nn.MSELoss() # Loss function (MSE)\n",
    "optimizer =  torch.optim.SGD(model.parameters(), lr=0.01)  # Optimizer (SGD)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3000):\n",
    "    model.train()\n",
    "\n",
    "    # Forward pass\n",
    "    y_pred = model(x_train)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = criterion(y_pred, y_train)  # Compute loss using criterion\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    y_test_pred = model(x_test)  # Get predictions for the test set\n",
    "    test_loss = criterion(y_test_pred, y_test)  # Compute test loss\n",
    "\n",
    "    print(f'Test Loss: {test_loss.item()}')\n",
    "\n",
    "# Show some final predictions\n",
    "print(\"Final Predictions (first 5 test samples):\")\n",
    "for i in range(5):\n",
    "    print(f\"Predicted: {y_test_pred[i].item():.4f}, Actual: {y_test[i].item():.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
